\documentclass[11pt,a4paper]{scrartcl}
\usepackage[margin=2cm]{geometry}
\setlength{\parskip}{5pt}
% Encoding
\usepackage[utf8]{inputenc}
% Bibliography
\usepackage[backend=biber,bibstyle=ieee,citestyle=numeric]{biblatex}
\bibliography{sps_cw1}
\usepackage[hidelinks]{hyperref} % Clickable links
% Header
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{COMS21202 Symbols, Patterns and Signals - CW1}
\rhead{}
%Figures
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{subfig}
%Per page footnote numbering
\usepackage[perpage]{footmisc}
% Maths
\usepackage{amsmath}
\usepackage{bm}

%Code formatting
\usepackage{listings}
\usepackage{xcolor}
\definecolor{code-comment}{rgb}{0,0.6,0}
\definecolor{code-string}{rgb}{0.58,0,0.82}
\lstset{
	frame=tb,
	language=Python,
	keywordstyle=\color{blue},
	stringstyle=\color{code-string},
	commentstyle=\color{code-comment},
	tabsize=4,
	basicstyle={\small\ttfamily},
	showstringspaces=false,
	breaklines=true,
	aboveskip=3mm,
	belowskip=0mm,
}

\title{An Unknown Signal}
\subtitle{Symbols, Patterns and Signals}
\author{Jacob Daniel Halsey}
\date{March 2020}

\begin{document}
	
\maketitle
	
\section{Least Squares Regression}

The least squares calculations have been implemented in the \lstinline|Segment| methods \lstinline|lsr_polynomial()| and \lstinline|lsr_fn()|.
They both use the matrix formula: \cite{wolfram_ls_poly}

\[\bm{A}=(\bm{X}^{T}\bm{X})^{-1}\bm{X}^{T}\bm{Y}\]

Where in the case of the polynomial regression:

\[
\bm{X} = \begin{bmatrix}
1 & x_{0} & \left (x_{0}  \right )^{2} & .. & \left (x_{0}  \right )^{k} \\ 
1 & x_{1} & \left (x_{1}  \right )^{2} & .. & \left (x_{1}  \right )^{k} \\ 
.. & .. & .. & .. & .. \\ 
1 & x_{k} & \left (x_{k}  \right )^{2} & .. & \left (x_{k}  \right )^{k}
\end{bmatrix}\]

Or in the case of the arbitrary function ($f$) regression:
\[
\bm{X} = 
\begin{bmatrix}
1 & f\left (x_{0}  \right ) \\ 
1 & f\left (x_{1}  \right ) \\ 
.. & .. \\ 
1 & f\left (x_{k}  \right )
\end{bmatrix}
\]

$\bm{Y}$ is a vector containing the $y$ values $y_{0}$ through to $y_{k}$

And the result $\bm{A}$ is the list of coefficients, in the order $a_{0} + a_{1}x + .. + a_{k}x^{k}$ or $a_{0} + a_{1}f(x)$.

\subsection*{Calculating the Error}

The Sum Squared Error (SSE) is a method of measuring how well a fitted function ($f$) fits a dataset of $n$ points, by calculating the difference between the predicted and actual data. \cite{wolfram_ls_fitting}

\[SSE = \sum_{i=0}^{n} ( y_{i} - f(x_{i}) )^{2}\]

This has been implemented in the \lstinline|ss_error()| function, which is called with an array of predicted $y$ values, and array of actual $y$ values.

\section{Overfitting}

Whilst choosing the function with the lowest SSE will find the best fit for the given dataset, it does not prevent overfitting, a type of error where the model fits too closely to the noise in the data, and is therefore unable to accurately predict additional observations. In our case this can be caused by selecting a function class too complex, such as a higher order polynomial than the real function. Cross validation is a technique that can be used to detect this; first we split the data into training and validation sets, then fit a function to the training data, and finally measure the SSE of the validation data when using the fitted function, giving us the cross validation error \cite{lecture_overfitting}.

K-Fold cross validation is a method of applying cross validation repeatedly across different combinations of training and validation data. To do so we split our data into $K$ equally sized chunks, for $1..K$ each chunk is used as a validation set, and the other chunks form the training set \cite{k_fold_validation}. This has been implemented by the \lstinline|Segment| \lstinline|split()| method which returns a list of unique training/validation pairs of length $K$. This in turn is called by the \lstinline|cross_validated()| method which computes the mean validation error (CVE) for each of the pairs.

\section{Method}

The \lstinline|compute()| function is used to find the best fitting function for a segment out of three choices: a linear function, a polynomial of given degree, or a specified custom function. To prevent overfitting it selects the function with the minimum k-fold cross validation. The importance of this is demonstrated by figure \ref{fig:noise_1}, which shows that if we just use the minimum SSE, a higher order polynomial will be incorrectly favoured instead of a linear function; due to the noise in the data.

\begin{figure}
	\centering
	\subfloat[Minimum CVE: Linear]{{\includegraphics[width=7.6cm]{noise_1_min_cve} }}
	\qquad
	\subfloat[Minimum SSE: Cubic]{{\includegraphics[width=7.6cm]{noise_1_min_sse} }}
	\caption{\lstinline|noise_1.csv| fitted with different minimums}
	\label{fig:noise_1}
\end{figure}

\section{Testing}

To verify that the mathematical calculations have been implemented correctly, I have written tests using the \lstinline|unittest| framework from the Python standard library \cite{unittest}. I obtained test data for the regressions and sum square error using a \textit{TI-Nspire} graphing calculator. There is complete test coverage of the \lstinline|Segment| class and \lstinline|compute()| function.


\section{Training Data}

The final part of the task was to establish which polynomial degree and which unknown function were used in the training data. To do so I have written the \lstinline|evaluate_training_data()| function, which can be executed by running the script with the \lstinline|--evaluate| switch. I have defined a range of polynomials from quadratic to sixth degree, and a range of candidates for the unknown function including $sin$, $cos$, $tan$, $reciprocal$, $exp$, $log$, $square$ and $sqrt$. The evaluation iterates over all possible combinations of polynomial and unknown function, applying the \lstinline|compute()| function to all training segments, totalling the cross validation error from each segment.

The results in ascending order were as follows:

\begin{lstlisting}
	Total CV Error: 1364.7933944444837, Polynomial Degree: 2, Function: sin
	Total CV Error: 7417.386188226556, Polynomial Degree: 3, Function: sin
	Total CV Error: 92417.36032612705, Polynomial Degree: 3, Function: cos
	...
\end{lstlisting}

I therefore believe that the quadratic polynomial and $sin$ function were used when generating the training data. I have defined them as constants which will now be used when script is run on a \lstinline|csv| file.

\printbibliography


\end{document}
